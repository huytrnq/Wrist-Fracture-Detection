{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GiSQFoSZ5zpZ"
      },
      "outputs": [],
      "source": [
        "## Import system library\n",
        "import os\n",
        "import glob\n",
        "\n",
        "## Import Image Processing library\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "from skimage import exposure\n",
        "from skimage.color import rgb2gray\n",
        "from scipy.ndimage import convolve, uniform_filter\n",
        "from skimage.filters import gabor_kernel, unsharp_mask, threshold_otsu, gabor\n",
        "from skimage.morphology import disk, closing, dilation\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "## Import machine learning library\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from skimage.feature import hog\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "from utils.lbp import LBP\n",
        "from utils.vis import draw_bboxes\n",
        "from utils.preprocess import sliding_window\n",
        "from utils.dataset import load_yolo_labels\n",
        "from utils.bboxes import iou, calculate_boxA_percentage\n",
        "from utils.intensity_transforms import histogram_matching, calculate_mean_histogram\n",
        "from utils.dataset import adjust_labels_for_pooling, resize_image_and_bboxes\n",
        "from models.kernels import AlexNetDescriptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IPxEOjKy7bT7"
      },
      "outputs": [],
      "source": [
        "## Defind dataset folders\n",
        "\n",
        "# root_path = '/content/drive/Shareddrives/Wrist_fracture_detectiom/ML/Dataset'\n",
        "root_path = 'MLDataset/crop_data'\n",
        "\n",
        "img_train_folder = 'train'\n",
        "img_test_folder = 'test'\n",
        "label_folder = 'labels'\n",
        "image_folder = 'images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_features(export_path, name, feature_name, feature_list):\n",
        "    \"\"\"Save features to npy file\n",
        "\n",
        "    Args:\n",
        "        export_path (str): path to save the file\n",
        "        name (str): file name to save\n",
        "        feature_name (str): name of the feature. Ex: 'lbp', 'hog', 'gabor'\n",
        "        feature_list (list/array): list of features\n",
        "    \"\"\"\n",
        "    ## Export features to file\n",
        "    save_name = f'{name}_{feature_name}.npy'\n",
        "    save_path = os.path.join(export_path, save_name)\n",
        "    np.save(save_path, feature_list)\n",
        "    print(f'Save {feature_name} features to {save_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = 'MLDataset/crop_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def undersample_data(features, labels):\n",
        "    \"\"\"Undersample the data\n",
        "\n",
        "    Args:\n",
        "        features (np.array): features\n",
        "        labels (np.array): labels\n",
        "\n",
        "    Returns:\n",
        "        np.array: undersampled features\n",
        "        np.array: undersampled labels\n",
        "    \"\"\"\n",
        "    # Find the number of samples in each class\n",
        "    X_majority = features[labels == 0]\n",
        "    X_minority = features[labels == 1]\n",
        "    y_majority = labels[labels == 0]\n",
        "    y_minority = labels[labels == 1]\n",
        "    \n",
        "    # Downsample the majority class\n",
        "    X_majority_downsampled, y_majority_downsampled = resample(X_majority, y_majority, replace=False, n_samples=len(X_minority), random_state=42)\n",
        "    \n",
        "    # Combine the minority class with the downsampled majority class\n",
        "    X_downsampled = np.concatenate([X_majority_downsampled, X_minority])\n",
        "    y_downsampled = np.concatenate([y_majority_downsampled, y_minority])\n",
        "    \n",
        "    return X_downsampled, y_downsampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature_list = ['hog', 'alex']\n",
        "# feature_list =['hog']\n",
        "# feature_list =['hog_canny']\n",
        "# feature_list =['lbp']\n",
        "feature_list =['alex']\n",
        "\n",
        "train_features_list = []\n",
        "train_labels = None\n",
        "test_features_list = []\n",
        "test_labels = None\n",
        "\n",
        "for feature_name in feature_list:\n",
        "    # Load the dictionary from the .npz file\n",
        "    train_fracture = np.load(os.path.join(root_path, f'train_fracture_{feature_name}.npy'), allow_pickle=True)\n",
        "    train_normal = np.load(os.path.join(root_path, f'train_normal_{feature_name}.npy'), allow_pickle=True)\n",
        "    train_fracture_labels = np.load(os.path.join(root_path, f'train_fracture_labels_{feature_name}.npy'), allow_pickle=True)\n",
        "    train_normal_labels = np.load(os.path.join(root_path, f'train_normal_labels_{feature_name}.npy'), allow_pickle=True)\n",
        "\n",
        "    test_fracture = np.load(os.path.join(root_path, f'test_fracture_{feature_name}.npy'), allow_pickle=True)\n",
        "    test_normal = np.load(os.path.join(root_path, f'test_normal_{feature_name}.npy'), allow_pickle=True)\n",
        "    test_fracture_labels = np.load(os.path.join(root_path, f'test_fracture_labels_{feature_name}.npy'), allow_pickle=True)\n",
        "    test_normal_labels = np.load(os.path.join(root_path, f'test_normal_labels_{feature_name}.npy'), allow_pickle=True)\n",
        "\n",
        "    train_features = np.concatenate([train_fracture, train_normal])\n",
        "    train_labels= np.concatenate([train_fracture_labels, train_normal_labels])\n",
        "    train_indices = np.where(train_labels!= -1)[0]\n",
        "    train_features = train_features[train_indices]\n",
        "    train_labels= train_labels[train_indices]\n",
        "\n",
        "    test_features= np.concatenate([test_fracture, test_normal])\n",
        "    test_labels = np.concatenate([test_fracture_labels, test_normal_labels])\n",
        "    test_indices = np.where(test_labels!= -1)[0]\n",
        "    test_features = test_features[test_indices]\n",
        "    test_labels = test_labels[test_indices]\n",
        "    \n",
        "    # Append to the list\n",
        "    train_features_list.append(train_features)\n",
        "    test_features_list.append(test_features)\n",
        "\n",
        "# Concatenate the features\n",
        "train_features = np.concatenate(train_features_list, axis=1)\n",
        "test_features = np.concatenate(test_features_list, axis=1)\n",
        "\n",
        "train_features, train_labels = undersample_data(train_features, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "hog_train_features = train_features\n",
        "hog_test_features = test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "hog_canny_train_features = train_features\n",
        "hog_canny_test_features = test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "lbp_train_features = train_features\n",
        "lbp_test_features = test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "alex_train_features = train_features\n",
        "alex_test_features = test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.71      0.74       572\n",
            "           1       0.71      0.78      0.74       532\n",
            "\n",
            "    accuracy                           0.74      1104\n",
            "   macro avg       0.74      0.74      0.74      1104\n",
            "weighted avg       0.74      0.74      0.74      1104\n",
            "\n",
            "Confusion Matrix\n",
            "[[405 167]\n",
            " [119 413]]\n"
          ]
        }
      ],
      "source": [
        "# Concatenate Features\n",
        "X = np.concatenate([hog_train_features, hog_canny_train_features, lbp_train_features, alex_train_features], axis=1)\n",
        "y = train_labels  # Labels for the images\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a classifier on the concatenated features\n",
        "classifier = XGBClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test, y_pred))\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "hog_X_train, hog_X_test, hog_y_train, hog_y_test = train_test_split(hog_train_features, train_labels, test_size=0.3, random_state=42)\n",
        "hog_canny_X_train, hog_canny_X_test, hog_canny_y_train, hog_canny_y_test = train_test_split(hog_canny_train_features, train_labels, test_size=0.3, random_state=42)\n",
        "lbp_X_train, lbp_X_test, lbp_y_train, lbp_y_test = train_test_split(lbp_train_features, train_labels, test_size=0.3, random_state=42)\n",
        "alex_X_train, alex_X_test, alex_y_train, alex_y_test = train_test_split(alex_train_features, train_labels, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 1308, number of negative: 1268\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003986 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 36717\n",
            "[LightGBM] [Info] Number of data points in the train set: 2576, number of used features: 144\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507764 -> initscore=0.031058\n",
            "[LightGBM] [Info] Start training from score 0.031058\n",
            "[LightGBM] [Info] Number of positive: 1308, number of negative: 1268\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12495\n",
            "[LightGBM] [Info] Number of data points in the train set: 2576, number of used features: 49\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507764 -> initscore=0.031058\n",
            "[LightGBM] [Info] Start training from score 0.031058\n"
          ]
        }
      ],
      "source": [
        "# Feature-Level Ensemble\n",
        "hog_model = XGBClassifier(n_estimators=100, random_state=42).fit(hog_X_train, hog_y_train)\n",
        "hog_canny_model = LGBMClassifier(n_estimators=100, random_state=42).fit(hog_canny_X_train, hog_canny_y_train)\n",
        "lbp_model = XGBClassifier(n_estimators=100, random_state=42).fit(lbp_X_train, lbp_y_train)\n",
        "alex_model = LGBMClassifier(n_estimators=100, random_state=42).fit(alex_X_train, alex_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "# Combine predictions (Voting Classifier as an example)\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('hog', hog_model), ('hog_canny', hog_canny_model), ('lbp', lbp_model), ('alex', alex_model)], voting='hard')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 1308, number of negative: 1268\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021553 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205796\n",
            "[LightGBM] [Info] Number of data points in the train set: 2576, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507764 -> initscore=0.031058\n",
            "[LightGBM] [Info] Start training from score 0.031058\n",
            "[LightGBM] [Info] Number of positive: 1308, number of negative: 1268\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023694 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205796\n",
            "[LightGBM] [Info] Number of data points in the train set: 2576, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507764 -> initscore=0.031058\n",
            "[LightGBM] [Info] Start training from score 0.031058\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.71      0.74       572\n",
            "           1       0.71      0.78      0.74       532\n",
            "\n",
            "    accuracy                           0.74      1104\n",
            "   macro avg       0.74      0.74      0.74      1104\n",
            "weighted avg       0.74      0.74      0.74      1104\n",
            "\n",
            "Confusion Matrix\n",
            "[[405 167]\n",
            " [119 413]]\n"
          ]
        }
      ],
      "source": [
        "voting_clf.fit(np.hstack([hog_X_train, hog_canny_X_train, lbp_X_train, alex_X_train]), y_train)\n",
        "y_pred_ensemble = voting_clf.predict(np.hstack([hog_X_test, hog_canny_X_test, lbp_X_test, alex_X_test]))\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_test, y_pred_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.72      0.81      2019\n",
            "           1       0.38      0.76      0.51       453\n",
            "\n",
            "    accuracy                           0.73      2472\n",
            "   macro avg       0.66      0.74      0.66      2472\n",
            "weighted avg       0.83      0.73      0.76      2472\n",
            "\n",
            "Confusion Matrix\n",
            "[[1453  566]\n",
            " [ 107  346]]\n"
          ]
        }
      ],
      "source": [
        "y_pred_ensemble = voting_clf.predict(np.hstack([hog_test_features, hog_canny_test_features, lbp_test_features, alex_test_features]))\n",
        "print('Classification Report')\n",
        "print(classification_report(test_labels, y_pred_ensemble))\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_labels, y_pred_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 1308, number of negative: 1268\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022033 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205796\n",
            "[LightGBM] [Info] Number of data points in the train set: 2576, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507764 -> initscore=0.031058\n",
            "[LightGBM] [Info] Start training from score 0.031058\n",
            "[LightGBM] [Info] Number of positive: 1308, number of negative: 1268\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023828 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205796\n",
            "[LightGBM] [Info] Number of data points in the train set: 2576, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507764 -> initscore=0.031058\n",
            "[LightGBM] [Info] Start training from score 0.031058\n",
            "[LightGBM] [Info] Number of positive: 1046, number of negative: 1014\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019644 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205776\n",
            "[LightGBM] [Info] Number of data points in the train set: 2060, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507767 -> initscore=0.031070\n",
            "[LightGBM] [Info] Start training from score 0.031070\n",
            "[LightGBM] [Info] Number of positive: 1046, number of negative: 1015\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019922 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205782\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507521 -> initscore=0.030085\n",
            "[LightGBM] [Info] Start training from score 0.030085\n",
            "[LightGBM] [Info] Number of positive: 1046, number of negative: 1015\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018286 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205781\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507521 -> initscore=0.030085\n",
            "[LightGBM] [Info] Start training from score 0.030085\n",
            "[LightGBM] [Info] Number of positive: 1047, number of negative: 1014\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018173 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205786\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508006 -> initscore=0.032026\n",
            "[LightGBM] [Info] Start training from score 0.032026\n",
            "[LightGBM] [Info] Number of positive: 1047, number of negative: 1014\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019626 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205783\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508006 -> initscore=0.032026\n",
            "[LightGBM] [Info] Start training from score 0.032026\n",
            "[LightGBM] [Info] Number of positive: 1046, number of negative: 1014\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018210 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205776\n",
            "[LightGBM] [Info] Number of data points in the train set: 2060, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507767 -> initscore=0.031070\n",
            "[LightGBM] [Info] Start training from score 0.031070\n",
            "[LightGBM] [Info] Number of positive: 1046, number of negative: 1015\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017902 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205782\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507521 -> initscore=0.030085\n",
            "[LightGBM] [Info] Start training from score 0.030085\n",
            "[LightGBM] [Info] Number of positive: 1046, number of negative: 1015\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018858 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205781\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507521 -> initscore=0.030085\n",
            "[LightGBM] [Info] Start training from score 0.030085\n",
            "[LightGBM] [Info] Number of positive: 1047, number of negative: 1014\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018061 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205786\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508006 -> initscore=0.032026\n",
            "[LightGBM] [Info] Start training from score 0.032026\n",
            "[LightGBM] [Info] Number of positive: 1047, number of negative: 1014\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018128 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205783\n",
            "[LightGBM] [Info] Number of data points in the train set: 2061, number of used features: 1793\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508006 -> initscore=0.032026\n",
            "[LightGBM] [Info] Start training from score 0.032026\n",
            "Stacking Ensemble Accuracy: 0.595108695652174\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.58      0.60       572\n",
            "           1       0.58      0.61      0.59       532\n",
            "\n",
            "    accuracy                           0.60      1104\n",
            "   macro avg       0.60      0.60      0.60      1104\n",
            "weighted avg       0.60      0.60      0.60      1104\n",
            "\n",
            "Confusion Matrix\n",
            "[[333 239]\n",
            " [208 324]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/huytrq/miniconda3/envs/py11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Model-Level Ensemble (Stacking)\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define base models\n",
        "base_learners = [\n",
        "    ('hog', XGBClassifier(n_estimators=100, random_state=42)),\n",
        "    ('hog_canny', LGBMClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lbp', XGBClassifier(n_estimators=100, random_state=42)),\n",
        "    ('alex', LGBMClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Define meta learner\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Create stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, passthrough=True)\n",
        "\n",
        "# Train stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "print('Stacking Ensemble Accuracy:', accuracy_score(y_test, y_pred_stacking))\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test, y_pred_stacking))\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_test, y_pred_stacking))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stacking Ensemble Accuracy: 0.6023462783171522\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.61      0.72      2019\n",
            "           1       0.24      0.56      0.34       453\n",
            "\n",
            "    accuracy                           0.60      2472\n",
            "   macro avg       0.55      0.59      0.53      2472\n",
            "weighted avg       0.75      0.60      0.65      2472\n",
            "\n",
            "Confusion Matrix\n",
            "[[1235  784]\n",
            " [ 199  254]]\n"
          ]
        }
      ],
      "source": [
        "test_features_all = np.concatenate([hog_test_features, hog_canny_test_features, lbp_test_features, alex_test_features], axis=1)\n",
        "y_pred_stacking = stacking_clf.predict(test_features_all)\n",
        "print('Stacking Ensemble Accuracy:', accuracy_score(test_labels, y_pred_stacking))\n",
        "print('Classification Report')\n",
        "print(classification_report(test_labels, y_pred_stacking))\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_labels, y_pred_stacking))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
